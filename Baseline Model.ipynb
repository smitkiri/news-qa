{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A baseline model\n",
    "\n",
    "Let's first create a baseline model using the \"Software 1.0\" approach where we take a pre-defined rule based approach and there's no active \"learning\". \n",
    "\n",
    "In this approach, we compare the cosine similarity of the question with each sentence in the text. \n",
    "\n",
    "- The sentences are first tokenized and any stop words or URLs are removed. \n",
    "- Then, an embedding for each sentence is computed by taking the average of GloVe embeddings of all tokens in the sentence.\n",
    "- These \"sentence-embeddings\" are compared to the \"question-embedding\" using cosine similarity and the most similar sentence is chosen as the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "NEWS_STORIES = utils.open_pickle('data/news_stories.pkl')\n",
    "data = pd.read_csv('data/newsqa-dataset-cleaned.csv')\n",
    "total_examples = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc, model=en):\n",
    "    # a simple tokenizer for individual documents\n",
    "    parsed = model(doc)\n",
    "    return([t.lower_ for t in parsed if (t.is_alpha)&(not t.like_url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_embedding(tokens, model = nlp):\n",
    "    '''\n",
    "    Returns the embedding of a document by averaging the\n",
    "    GloVe embeddings of all tokens in the document\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    tokens: list\n",
    "            The list of document tokens\n",
    "    \n",
    "    model: The model to use for getting embeddings\n",
    "    '''\n",
    "    embeddings = []\n",
    "    for t in tokens:\n",
    "        embeddings.append(model.vocab[t].vector)\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    if embeddings.ndim == 1:\n",
    "        return embeddings\n",
    "    else:\n",
    "        return np.mean(embeddings, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(text, question):\n",
    "    '''\n",
    "    Returns the start and end indices of the sentence that\n",
    "    has the maximum cosine similarity with the question\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    text: str\n",
    "          The text to find the answer in\n",
    "    \n",
    "    question: str\n",
    "              The question to answer\n",
    "    '''\n",
    "    # Stores the start position of each sentence\n",
    "    sentence_to_char_idx = [0]\n",
    "    \n",
    "    sentences = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for idx, char in enumerate(text):\n",
    "        # If the chracter is a punctuation, we append the sentence\n",
    "        if utils.is_punct(char):\n",
    "            sentences.append(text[start_idx:idx])\n",
    "            start_idx = idx + 1\n",
    "            sentence_to_char_idx.append(start_idx)\n",
    "    \n",
    "    # Getting embeddings for each sentence\n",
    "    sentence_embeddings = []\n",
    "    for s in sentences:\n",
    "        tokens = simple_tokenizer(s)\n",
    "        embd = get_doc_embedding(tokens)\n",
    "        if embd.shape == (300,):\n",
    "            sentence_embeddings.append(embd)\n",
    "    \n",
    "    sentence_embeddings = np.stack(sentence_embeddings)\n",
    "    \n",
    "    # Getting the embedding for the question\n",
    "    question_embedding = get_doc_embedding(simple_tokenizer(question))\n",
    "    question_embedding = np.expand_dims(question_embedding, axis = 0)\n",
    "    \n",
    "    #print(sentence_embeddings.shape)\n",
    "    # Get the cosine similarity of each sentence with the question\n",
    "    similarity = cosine_similarity(sentence_embeddings, question_embedding)\n",
    "    \n",
    "    # Get the sentence with the most similarity\n",
    "    best_idx = np.argmax(similarity)\n",
    "    \n",
    "    # Get the sentence start and end index\n",
    "    pred_start = sentence_to_char_idx[best_idx]\n",
    "    pred_end = sentence_to_char_idx[best_idx + 1] - 1\n",
    "    \n",
    "    return pred_start, pred_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics:\n",
    "- **Accuracy:** If the predicted answer overlaps with the actual answer at all, it is considered as correct. $$accuracy = \\frac{num\\_correct}{num\\_total}$$\n",
    "\n",
    "\n",
    "- **F1 score:** Overlap is calculated as the common number of characters in the predicted answer and the actual answer. $$precision = \\frac{overlap}{pred\\_ans\\_len}$$\n",
    "\n",
    "$$recall = \\frac{overlap}{actual\\_ans\\_len}$$\n",
    "\n",
    "$$f1\\_score = \\frac{2*precision*recall}{precision + recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(pred_start, pred_end, true_start, true_end):\n",
    "    '''\n",
    "    Calculates the f1 score and if the predicted answer overlaps \n",
    "    with the correct one\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    pred_start, pred_end: int\n",
    "                          The predicted start and end indices\n",
    "\n",
    "    true_start, true_end: int\n",
    "                          The actual indices\n",
    "    '''\n",
    "    # Get the overlap\n",
    "    overlap = set(range(true_start, true_end)).intersection(range(pred_start, pred_end))\n",
    "    overlap = len(overlap)\n",
    "\n",
    "    # If either of them have no answer\n",
    "    if true_end == 0 or pred_end == 0:\n",
    "        f1_score = int(true_end == pred_end)\n",
    "        is_correct = int(end_idx == pred_end)\n",
    "        return f1_score, is_correct\n",
    "    \n",
    "    # If they don't overlap at all\n",
    "    if overlap == 0 or pred_start >= pred_end:\n",
    "        f1_score = 0\n",
    "        is_correct = 0\n",
    "        return f1_score, is_correct\n",
    "\n",
    "    # If there is an overlap, we consider it correct\n",
    "    is_correct = 1\n",
    "\n",
    "    precision = overlap / (pred_end - pred_start)\n",
    "    recall = overlap / (true_end - true_start)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return f1_score, is_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no \"training\" happening in this approach, so we will evaluate the results on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [====================] 87810/87810"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of this approach on the data\n",
    "correct = 0\n",
    "total_f1 = 0\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    text = NEWS_STORIES[row['story_id']]\n",
    "    question = row['question']\n",
    "    \n",
    "    # Get the predictions\n",
    "    pred_start, pred_end = predict_answer(text, question)\n",
    "    f1, is_correct = calculate_metrics(pred_start, pred_end, row['start_idx'], row['end_idx'])\n",
    "    \n",
    "    total_f1 += f1\n",
    "    correct += is_correct\n",
    "    \n",
    "    # Print progress\n",
    "    utils.drawProgressBar(idx + 1, total_examples)\n",
    "    \n",
    "acc = correct/total_examples\n",
    "f1_score = total_f1/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.0408\n",
      "Accuracy: 0.1261\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score: {:.4f}\".format(f1_score))\n",
    "print(\"Accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is very low, which was expected as the actual answer do not span an entire sentence, whereas the predicted answer will be a full sentence. But it is quite interesting to see that this approach was able to predict the sentence where the answer lies nealy 12.6% of the time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
